{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.datasets import fashion_mnist\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layers:\n",
    "    def __init__(self,input_dim,output_dim,activation=\"sigmoid\"): \n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.activation = activation\n",
    "        self.W = np.random.randn(output_dim,input_dim) * 1\n",
    "        self.b = np.zeros((output_dim,1))\n",
    "        self.db = np.zeros((output_dim,1))\n",
    "        self.H = None\n",
    "        self.A = None\n",
    "        self.dW = np.zeros((output_dim,input_dim))\n",
    "        self.dH = None\n",
    "    def activated_values(self,X):\n",
    "        if self.activation == \"sigmoid\":\n",
    "            A = np.clip(X, -700, 700)\n",
    "            sig = 1 / (1 + np.exp(-A))\n",
    "            return sig \n",
    "        if self.activation == \"ReLU\":\n",
    "            relu = lambda x: x if x > 0 else 0\n",
    "            return np.vectorize(relu)(X)\n",
    "        if self.activation == \"linear\":\n",
    "            return X \n",
    "        if self.activation == \"Softmax\":\n",
    "            A = np.clip(X, -700, 700)  # Clipping to avoid overflow\n",
    "            exps = np.exp(A - np.max(A, axis=0, keepdims=True))\n",
    "            return exps / np.sum(exps, axis=0, keepdims=True)\n",
    "        \n",
    "    def dactivation_da(self,X):\n",
    "        if self.activation == \"sigmoid\":\n",
    "            A = np.clip(X, -500, 500)\n",
    "            return 1/(1+np.exp(-A)) * (1 - 1/(1+np.exp(-A)))\n",
    "        if self.activation == \"ReLU\":\n",
    "            return np.where(X > 0, 1, 0)\n",
    "        if self.activation == \"linear\":\n",
    "            return np.ones_like(X)\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        self.uW = []\n",
    "        self.ub = []\n",
    "        for layer in layers:\n",
    "            self.uW.append(np.zeros((layer.dW.shape[0],layer.dW.shape[1])))\n",
    "            self.ub.append(np.zeros((layer.db.shape[0],layer.db.shape[1])))\n",
    "    \n",
    "    def forward(self,X):\n",
    "        # X = X.reshape(-1,1)\n",
    "        self.layers[0].A = self.layers[0].W @ X + self.layers[0].b \n",
    "        self.layers[0].H = self.layers[0].activated_values(self.layers[0].A)\n",
    "        for i in range(1,len(self.layers)):\n",
    "            self.layers[i].A = self.layers[i].W @ self.layers[i-1].H + self.layers[i].b\n",
    "            self.layers[i].H = self.layers[i].activated_values(self.layers[i].A)\n",
    "        return self.layers[i].H\n",
    "    \n",
    "    def backPropagation(self,X,Y):\n",
    "        m = X.shape[0]\n",
    "        encoded_Y = np.zeros((Y.shape[0],10))\n",
    "        encoded_Y[np.arange(Y.shape[0]),Y] = 1\n",
    "        n = len(self.layers)\n",
    "        for i in range(n):\n",
    "            self.layers[i].dW = np.zeros_like(self.layers[i].dW) \n",
    "            self.layers[i].db = np.zeros_like(self.layers[i].db)\n",
    "        # for x, encoded_y in zip(X, encoded_Y):\n",
    "            # x = x.reshape(-1,1)\n",
    "        \n",
    "        y_pred = self.forward(X.T)\n",
    "        dL_da = - (encoded_Y.T - y_pred)\n",
    "        for i in range(n-1,0,-1):\n",
    "            self.layers[i].dW =  dL_da @(self.layers[i-1].H).T / m\n",
    "            self.layers[i].db = np.sum(dL_da, axis=1, keepdims=True) / m\n",
    "            dL_dh = (self.layers[i].W).T @ dL_da\n",
    "            dL_da = dL_dh * self.layers[i-1].dactivation_da(self.layers[i-1].A)\n",
    "        self.layers[0].dW = dL_da @ (X) / m\n",
    "        self.layers[0].db = np.sum(dL_da, axis=1, keepdims=True) / m\n",
    "\n",
    "    def update_step(self,X,Y,beta = 0,optimiser = \"momemtum\"):\n",
    "        if optimiser == \"momemtum\":\n",
    "            self.backPropagation(X,Y)\n",
    "            for i in range(len(self.layers)):\n",
    "                self.uW[i] = beta*self.uW[i] + self.layers[i].dW\n",
    "                self.ub[i] = beta*self.ub[i] + self.layers[i].db\n",
    "            \n",
    "        if optimiser == \"nesterov\":\n",
    "            weights,bias = [],[]\n",
    "            for i in range(len(self.layers)):\n",
    "                weights.append(self.layers[i].W.copy())\n",
    "                bias.append(self.layers[i].b.copy())\n",
    "                self.layers[i].W = self.layers[i].W - beta*self.uW[i]\n",
    "                self.layers[i].b = self.layers[i].b - beta*self.ub[i]\n",
    "            self.backPropagation(X,Y)\n",
    "            for i in range(len(self.layers)):\n",
    "                self.uW[i] = beta*self.uW[i] + self.layers[i].dW\n",
    "                self.ub[i] = beta*self.ub[i] + self.layers[i].db\n",
    "                self.layers[i].W = weights[i]\n",
    "                self.layers[i].b = bias[i]\n",
    "\n",
    "        if optimiser == \"RMSprop\":\n",
    "            epsilon = 1e-8\n",
    "            decay_rate = 0.9    \n",
    "            self.backPropagation(X,Y)\n",
    "            for i in range(len(self.layers)):\n",
    "                self.uW[i] = decay_rate*self.uW[i] + (1-decay_rate)*self.layers[i].dW**2\n",
    "                self.ub[i] = decay_rate*self.ub[i] + (1-decay_rate)*self.layers[i].db**2\n",
    "                self.layers[i].W = self.layers[i].W - beta*self.layers[i].dW/(np.sqrt(self.uW[i])+epsilon)\n",
    "                self.layers[i].b = self.layers[i].b - beta*self.layers[i].db/(np.sqrt(self.ub[i])+epsilon)\n",
    "\n",
    "            weights,bias = [],[]\n",
    "            for i in range(len(self.layers)):\n",
    "                weights.append(self.layers[i].W.copy())\n",
    "                bias.append(self.layers[i].b.copy())\n",
    "                self.layers[i].W = self.layers[i].W - beta*self.uW[i]\n",
    "                self.layers[i].b = self.layers[i].b - beta*self.ub[i]\n",
    "            self.backPropagation(X,Y)\n",
    "            for i in range(len(self.layers)):\n",
    "                self.uW[i] = beta*self.uW[i] + self.layers[i].dW\n",
    "                self.ub[i] = beta*self.ub[i] + self.layers[i].db\n",
    "                self.layers[i].W = weights[i]\n",
    "                self.layers[i].b = bias[i]\n",
    "\n",
    "    def gradient_descent(self, X, Y, beta, eta, optimiser, gradientDescent=\"Vanilla\", batch_size=32):\n",
    "        \n",
    "        if gradientDescent == \"Vanilla\":\n",
    "            self.update_step(X,Y,beta=beta,optimiser = optimiser)\n",
    "            for i in range(len(self.layers)):\n",
    "                self.layers[i].W = self.layers[i].W - eta*(self.uW[i])/len(X)\n",
    "                self.layers[i].b = self.layers[i].b - eta*(self.ub[i])/len(X)\n",
    "                \n",
    "        elif gradientDescent == \"Minibatch\":\n",
    "            m = X.shape[0]\n",
    "            for i in range(0, m, batch_size):\n",
    "                batchX = X[i:i+batch_size]\n",
    "                batchY = Y[i:i+batch_size]\n",
    "                self.backPropagation(batchX, batchY)\n",
    "                for j in range(len(self.layers)):\n",
    "                    self.layers[j].W -= eta * self.layers[j].dW\n",
    "                    self.layers[j].b -= eta * self.layers[j].db\n",
    "    \n",
    "    def fit(self,X,y,beta,eta,epochs,optimiser,gradientDescent,verbose):\n",
    "        for epoch in range(1,epochs+1):\n",
    "            self.gradient_descent(X,y,beta=beta,eta=eta,optimiser = optimiser,gradientDescent = gradientDescent)\n",
    "            if verbose:\n",
    "                y_pred = self.forward(X.T).argmax(axis=0)\n",
    "                accuracy = np.sum(y_pred==y)/len(y_pred)\n",
    "                print(f\"After epoch {epoch} the achieved accuracy is {accuracy}\")\n",
    "\n",
    "    def predict(self,X):\n",
    "        return self.forward(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1],\n",
       "       [ 3,  4],\n",
       "       [ 6,  7],\n",
       "       [ 9, 10],\n",
       "       [12, 13],\n",
       "       [15, 16],\n",
       "       [18, 19],\n",
       "       [21, 22],\n",
       "       [24, 25],\n",
       "       [27, 28]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = np.arange(20).reshape(-1,2)\n",
    "x = np.arange(10).reshape(-1,1)\n",
    "z+x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist_data = fashion_mnist.load_data()\n",
    "train_data_x,train_data_y, test_data_x, test_data_y = fashion_mnist_data[0][0], fashion_mnist_data[0][1], fashion_mnist_data[1][0], fashion_mnist_data[1][1]\n",
    "train_data_x = train_data_x.reshape(train_data_x.shape[0],-1)\n",
    "test_data_x = test_data_x.reshape(test_data_x.shape[0],-1)\n",
    "train_data_x, validation_data_x, train_data_y, validation_data_y = train_test_split(train_data_x, train_data_y, test_size=0.1)\n",
    "train_data_x = train_data_x / 255.0\n",
    "validation_data_x = validation_data_x / 255.0\n",
    "test_data_x = test_data_x / 255.0\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54000, 784)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_data_x.reshape(train_data_x.shape[0],-1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork([\n",
    "    Layers(784,100),\n",
    "    Layers(100,50),\n",
    "    Layers(50,10,activation=\"Softmax\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 1 the achieved accuracy is 0.7845925925925926\n",
      "After epoch 2 the achieved accuracy is 0.8227407407407408\n",
      "After epoch 3 the achieved accuracy is 0.8389074074074074\n"
     ]
    }
   ],
   "source": [
    "model.fit(X=train_data_x,y=train_data_y,beta=0,eta=.5,epochs=40,optimiser=\"momemtum\",gradientDescent=\"Minibatch\",verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.843"
      ]
     },
     "execution_count": 793,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.forward(test_data_x.T).argmax(axis=0)\n",
    "y_true = test_data_y\n",
    "accuracy = np.sum(y_pred==y_true)/len(y_pred)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_y.shape\n",
    "encoded_y = np.zeros((train_data_y.shape[0],np.max(train_data_y)+1))\n",
    "encoded_y[np.arange(train_data_y.shape[0]),train_data_y] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 791,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_y[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
